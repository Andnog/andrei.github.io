<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Transformers | Andrei Noguera</title>
    <link>http://localhost:1313/tags/transformers/</link>
      <atom:link href="http://localhost:1313/tags/transformers/index.xml" rel="self" type="application/rss+xml" />
    <description>Transformers</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Wed, 01 Dec 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>http://localhost:1313/media/icon_hu14089566703181701121.png</url>
      <title>Transformers</title>
      <link>http://localhost:1313/tags/transformers/</link>
    </image>
    
    <item>
      <title>Speech Generator with Transformers</title>
      <link>http://localhost:1313/project/speech-generator/</link>
      <pubDate>Wed, 01 Dec 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/project/speech-generator/</guid>
      <description>&lt;h2 id=&#34;project-overview&#34;&gt;Project Overview&lt;/h2&gt;
&lt;p&gt;This project, developed as part of the &amp;ldquo;Principles of Deep Learning&amp;rdquo; course in the Master&amp;rsquo;s in Data Science program, focused on building a speech generator using deep learning techniques, specifically transformers. The model was fine-tuned with presidential speeches and related materials to produce coherent text outputs.&lt;/p&gt;
&lt;h3 id=&#34;goals&#34;&gt;Goals&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Validate knowledge acquired in the course by applying deep learning techniques to solve a real-world problem.&lt;/li&gt;
&lt;li&gt;Use publicly available material to generate new text information.&lt;/li&gt;
&lt;li&gt;Train a deep learning model to generate speeches based on input text.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;main-objective&#34;&gt;Main Objective&lt;/h3&gt;
&lt;p&gt;To develop a deep learning model capable of generating speeches, trained on datasets including speeches from former Mexican presidents Felipe Calderón and Enrique Peña Nieto, and official press releases from presidential events.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;theoretical-framework&#34;&gt;Theoretical Framework&lt;/h2&gt;
&lt;h3 id=&#34;deep-learning&#34;&gt;Deep Learning&lt;/h3&gt;
&lt;p&gt;Deep learning is a subset of machine learning that utilizes neural networks with multiple layers to simulate human brain behavior. These networks are designed to process and analyze large datasets, enabling predictions and classifications.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;DL&#34; srcset=&#34;
               /project/speech-generator/resources/deeplearning_chart_hu2373431881331503144.webp 400w,
               /project/speech-generator/resources/deeplearning_chart_hu17305596546662484445.webp 760w,
               /project/speech-generator/resources/deeplearning_chart_hu14300440690594667113.webp 1200w&#34;
               src=&#34;http://localhost:1313/project/speech-generator/resources/deeplearning_chart_hu2373431881331503144.webp&#34;
               width=&#34;426&#34;
               height=&#34;412&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;transformers&#34;&gt;Transformers&lt;/h3&gt;
&lt;p&gt;Transformers are neural network architectures that excel in sequence-to-sequence tasks, such as language translation, text generation, and speech-to-text transformation. Unlike recurrent neural networks (RNNs), transformers leverage self-attention mechanisms to identify contextual relationships between words, allowing for faster and more parallelizable computations.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Transformers&#34; srcset=&#34;
               /project/speech-generator/resources/transformers_hu16626568564970676515.webp 400w,
               /project/speech-generator/resources/transformers_hu15729801764741662005.webp 760w,
               /project/speech-generator/resources/transformers_hu8283865407848857783.webp 1200w&#34;
               src=&#34;http://localhost:1313/project/speech-generator/resources/transformers_hu16626568564970676515.webp&#34;
               width=&#34;624&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;business-case&#34;&gt;Business Case&lt;/h2&gt;
&lt;p&gt;Natural Language Processing (NLP) is integral to Artificial Intelligence (AI), enabling machines to work with unstructured text data. Text generation, a challenging NLP application, has widespread utility in customer service (chatbots) and journalism (automated reporting). This project aimed to leverage transformers to create a tool capable of generating coherent speeches in Spanish, tailored to the dataset&amp;rsquo;s context.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;data-collection&#34;&gt;Data Collection&lt;/h2&gt;
&lt;p&gt;The dataset comprised 8,454 speech documents extracted from:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Official Mexican Government Sites&lt;/strong&gt;: Speeches and press releases from the presidency&amp;rsquo;s archives.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Wayback Machine&lt;/strong&gt;: Archived speeches from previous administrations.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The extraction process utilized Python libraries such as &lt;code&gt;BeautifulSoup&lt;/code&gt; and &lt;code&gt;requests&lt;/code&gt; to scrape and preprocess the data, which was stored as text files for model training.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;methodology&#34;&gt;Methodology&lt;/h2&gt;
&lt;p&gt;The project followed the CRISP-DM methodology, consisting of:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Business Understanding&lt;/strong&gt;: Researching existing text-generation methods and identifying use cases.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data Understanding&lt;/strong&gt;: Extracting and verifying speeches for training.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data Preparation&lt;/strong&gt;: Creating a corpus, tokenizing text, and building frequency dictionaries.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Modeling&lt;/strong&gt;: Evaluating deep learning frameworks (&lt;code&gt;PyTorch&lt;/code&gt;, &lt;code&gt;TensorFlow&lt;/code&gt;) and using pretrained transformer models.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Evaluation&lt;/strong&gt;: Measuring model performance using metrics like loss and manual inspection of generated text.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Methodology&#34; srcset=&#34;
               /project/speech-generator/resources/CRISP-DM_hu12083214195692133757.webp 400w,
               /project/speech-generator/resources/CRISP-DM_hu6600300787577855483.webp 760w,
               /project/speech-generator/resources/CRISP-DM_hu15258616431478168041.webp 1200w&#34;
               src=&#34;http://localhost:1313/project/speech-generator/resources/CRISP-DM_hu12083214195692133757.webp&#34;
               width=&#34;555&#34;
               height=&#34;550&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;models-implemented&#34;&gt;Models Implemented&lt;/h2&gt;
&lt;h3 id=&#34;model-1-pytorch&#34;&gt;Model 1: PyTorch&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Framework&lt;/strong&gt;: PyTorch&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pretrained Model&lt;/strong&gt;: Roberta&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Result&lt;/strong&gt;: Low loss but incoherent text generation.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;model-2-tensorflow&#34;&gt;Model 2: TensorFlow&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Framework&lt;/strong&gt;: TensorFlow&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pretrained Model&lt;/strong&gt;: AutoModel&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Result&lt;/strong&gt;: Improved coherence but repetitive text.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;model-3-gpt-2-fine-tuned&#34;&gt;Model 3: GPT-2 Fine-tuned&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Framework&lt;/strong&gt;: TensorFlow&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Configuration&lt;/strong&gt;: Medium GPT-2 model, fine-tuned with 1,000 epochs.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Result&lt;/strong&gt;: Most coherent and contextually relevant text outputs, though some results deviated from the input topic.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;final-model-configuration&#34;&gt;Final Model Configuration&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Model 3: GPT-2 Fine-tuned&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;gpt2&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;finetune&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sess&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;dataset&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;input.txt&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;model_name&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;355M&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;steps&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1000&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;restore_from&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;fresh&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;run_name&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;run1&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;print_every&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;50&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;sample_every&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;200&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;save_every&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;500&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;playground&#34;&gt;Playground&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Discursero Web Application&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;Discurselo&lt;/strong&gt; playground provides an interactive interface for generating speeches using artificial intelligence. Users can customize various parameters to create tailored and coherent texts based on their specific needs.&lt;/p&gt;
&lt;h3 id=&#34;features&#34;&gt;Features&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Input Field&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Users can enter a phrase or sentence to prompt the AI for speech generation.&lt;/li&gt;
&lt;li&gt;Example prompt: &lt;em&gt;&amp;ldquo;México necesita apoyo de&amp;hellip;&amp;rdquo;&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Output Display&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The generated speech is displayed in the output box, showing the result based on the input and configuration.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Customizable Settings&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Number of Characters&lt;/strong&gt;: Adjust the length of the generated speech.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Temperature&lt;/strong&gt;: Set the creativity of the text generation. A lower temperature produces deterministic results, while a higher value increases randomness and variability.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Number of Speeches&lt;/strong&gt;: Generate multiple outputs for comparison and selection.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Action Button&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A single button (&lt;em&gt;&amp;ldquo;Generar Discurso&amp;rdquo;&lt;/em&gt;) triggers the generation process, making it simple and intuitive for users.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Playground&#34; srcset=&#34;
               /project/speech-generator/resources/Playgroud_hu14645128099872404743.webp 400w,
               /project/speech-generator/resources/Playgroud_hu10518211611982399857.webp 760w,
               /project/speech-generator/resources/Playgroud_hu6982698190391628828.webp 1200w&#34;
               src=&#34;http://localhost:1313/project/speech-generator/resources/Playgroud_hu14645128099872404743.webp&#34;
               width=&#34;760&#34;
               height=&#34;395&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;sample-outputs&#34;&gt;Sample Outputs&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Prompt&lt;/strong&gt;: &amp;ldquo;El grave problema de inseguridad de México es&amp;rdquo;&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;

  &lt;span class=&#34;inline-block  pr-1&#34;&gt;
    &lt;svg style=&#34;height: 1em; transform: translateY(0.1em);&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;M7.5 8.25h9m-9 3H12m-9.75 1.51c0 1.6 1.123 2.994 2.707 3.227c1.129.166 2.27.293 3.423.379c.35.026.67.21.865.501L12 21l2.755-4.132a1.14 1.14 0 0 1 .865-.502a48.172 48.172 0 0 0 3.423-.379c1.584-.233 2.707-1.626 2.707-3.228V6.741c0-1.602-1.123-2.995-2.707-3.228A48.394 48.394 0 0 0 12 3c-2.392 0-4.744.175-7.043.513C3.373 3.746 2.25 5.14 2.25 6.741z&#34;/&gt;&lt;/svg&gt;
  &lt;/span&gt; Generated Text: &lt;hr class=&#34;m-0-imp&#34;&gt; 
&#34;El grave problema de inseguridad de México es claramente construir un México de leyes y de instituciones de seguridad.&lt;br&gt;&lt;br&gt;
Por supuesto, quisiera señalar que en la materia, no sólo eso, sino que a nivel nacional, por cierto, desde la especie de la democracia y de la voluntad política, de la libertad, de la democracia y de instituciones democráticas, puede ser una de las mejores, una de las medidas que hoy realizan, una de las mejores plataformas para que sea la mayor responsabilidad de la sociedad.&lt;br&gt;&lt;br&gt;
Asimismo, así como el hecho de que se han venido impulsando una mayor desarrollo en las instituciones de seguridad pública y de justicia, eso es lo que enfrentamos.&lt;br&gt;&lt;br&gt;
Muchas gracias.
&#34;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;/blockquote&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;&lt;strong&gt;Prompt&lt;/strong&gt;: &amp;ldquo;El compromiso del Estado Mexicano con las mujeres del país&amp;rdquo;&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;

  &lt;span class=&#34;inline-block  pr-1&#34;&gt;
    &lt;svg style=&#34;height: 1em; transform: translateY(0.1em);&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;M7.5 8.25h9m-9 3H12m-9.75 1.51c0 1.6 1.123 2.994 2.707 3.227c1.129.166 2.27.293 3.423.379c.35.026.67.21.865.501L12 21l2.755-4.132a1.14 1.14 0 0 1 .865-.502a48.172 48.172 0 0 0 3.423-.379c1.584-.233 2.707-1.626 2.707-3.228V6.741c0-1.602-1.123-2.995-2.707-3.228A48.394 48.394 0 0 0 12 3c-2.392 0-4.744.175-7.043.513C3.373 3.746 2.25 5.14 2.25 6.741z&#34;/&gt;&lt;/svg&gt;
  &lt;/span&gt; Generated Text: &lt;hr class=&#34;m-0-imp&#34;&gt; 
&#34;El compromiso del Estado Mexicano con las mujeres del país, es posible dar un gran camino para abrir a la gente la oportunidad de bienestar y el progreso de todas en el país.&lt;br&gt;&lt;br&gt;
Segundo. El México que se encuentra con la búsqueda de las niñas, los niños y los jóvenes, es un México de desarrollo social y de bienestar para todos.&lt;br&gt;&lt;br&gt;
Tercero. El México que encabeza por el que presenciamos el mes de julio de 2017, aprovechamos el esfuerzo de todos los niveles de Gobierno para contribuir a la construcción de una gente abierta.&lt;br&gt;&lt;br&gt;
Cuarto. El México que encabeza por el que encabeza el Presidente Enrique Peña Nieto, a través de los precios de generar los apoyos a los nuevos derechos fundamentales de los derechos humanos.&lt;br&gt;&lt;br&gt;
Hoy, se encuentra con la búsqueda de la gente, pues llegamos a los nuevos derechos fundamentales, y sigue siendo mejor.&#34;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The project demonstrated the potential of deep learning for text generation tasks. Key findings include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pretrained models save significant time and resources for training.&lt;/li&gt;
&lt;li&gt;Model coherence can be improved by narrowing training datasets to specific topics.&lt;/li&gt;
&lt;li&gt;Fine-tuning large-scale models like GPT-2 produces the best results but requires significant computational power.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;future-work&#34;&gt;Future Work&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Increase training epochs for better performance.&lt;/li&gt;
&lt;li&gt;Create topic-specific models to improve contextual relevance.&lt;/li&gt;
&lt;li&gt;Experiment with multilingual datasets for broader applicability.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;colaborations&#34;&gt;Colaborations&lt;/h3&gt;
&lt;p&gt;This project is a colaboration with Miguel Ángel Bernal&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
